{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Dataframe with Awkward Arrays\n",
    "For each event organize particles together in descending of Ef, pxf, pyf, and pzf order based on Ef. Extract columns for easy plotting: leading particle, sums of particle energies. Because of the type of interaction it can be relevant to subtract the mass of the strange baryons, protons and neutrons from the given energy. The calculated energy (Cal) columns represents this calculation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import awkward as ak\n",
    "from particle import Particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_pars(df, col):\n",
    "    '''From a given DataFrame return the largest number of particles from one event'''\n",
    "    \n",
    "    arr = df[col].to_numpy()\n",
    "    arr = ak.Array(arr)\n",
    "    counts = ak.num(arr)\n",
    "    \n",
    "    return np.max(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_value(arr, num, val):\n",
    "    '''Take in a given 2D awkward array and pad it with a value to a specified number'''\n",
    "    \n",
    "    awk = ak.Array(arr)\n",
    "    padded = ak.pad_none(awk, num, axis = 1)\n",
    "    arr = ak.fill_none(padded, val)\n",
    "    arr = ak.to_numpy(arr)\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_energy_df(df, pars, is_all, is_leading, is_count, isMom = False, num = 52, fill = 0):\n",
    "    '''Process of df and return a new dataframe of the designated columns for each particle in pars. '''\n",
    "    \n",
    "    # Pad each array\n",
    "    pdgf = pad_value(df['pdgf'].to_numpy(), num, 0)\n",
    "    ef = pad_value(df['Ef'].to_numpy(), num, 0)\n",
    "    kef = pad_value(df['kef'].to_numpy(), num, 0)\n",
    "    cthf = pad_value(df['cthf'].to_numpy(), num, 0)\n",
    "    \n",
    "    if(isMom):\n",
    "        pxf = pad_value(df['pxf'].to_numpy(), num, 0)\n",
    "        pyf = pad_value(df['pyf'].to_numpy(), num, 0)\n",
    "        pzf = pad_value(df['pzf'].to_numpy(), num, 0)\n",
    "    \n",
    "    # Create a column of new dataframe that specifies the event num\n",
    "    df_new = pd.DataFrame()\n",
    "    df_new['event'] = df.index\n",
    "    \n",
    "    # Go through each particle #\n",
    "    for par in pars:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # make arrays of energy, cthf, kef for each event\n",
    "        ef_par = np.where(pdgf == par, ef, fill)\n",
    "        ke_par = np.where(pdgf == par, kef, fill)\n",
    "        cthf_par = np.where(pdgf == par, cthf, fill)\n",
    "        \n",
    "        # make arrays of mometums if wanted\n",
    "        if(isMom):\n",
    "            pxf_par = np.where(pdgf == par, pxf, fill)\n",
    "            pyf_par = np.where(pdgf == par, pyf, fill)\n",
    "            pzf_par = np.where(pdgf == par, pzf, fill)\n",
    "        \n",
    "        ### SORT BASED ON EF ##\n",
    "        # Index to sort by \n",
    "        index = np.argsort(-ef_par)\n",
    "\n",
    "        # Sort ef, ke, angle\n",
    "        ef_par = np.take_along_axis(ef_par, index, axis=1)\n",
    "        ke_par = np.take_along_axis(ke_par, index, axis=1)\n",
    "        cthf_par = np.take_along_axis(cthf_par, index, axis=1)\n",
    "        \n",
    "        # Sort momentum if needed\n",
    "        if(isMom):\n",
    "            pxf_par = np.take_along_axis(pxf_par, index, axis=1)\n",
    "            pyf_par = np.take_along_axis(pyf_par, index, axis=1)\n",
    "            pzf_par = np.take_along_axis(pzf_par, index, axis=1)\n",
    "        \n",
    "        ## Remove zero momentum particles ##\n",
    "        # get rid of zeros in ef, kef, cthf (already zero in pxf, pyf, pzf)\n",
    "        if(fill == 0):\n",
    "            ef_par = np.where(pxf_par == 0, 0, ef_par)\n",
    "            kef_par = np.where(pxf_par == 0, 0, kef_par)\n",
    "            cthf_par = np.where(pxf_par == 0, 0, cthf_par)\n",
    "        else:\n",
    "            # Remove from all arrays if fill is not 0\n",
    "            ef_par = np.where(pxf_par == 0, fill, ef_par)\n",
    "            pxf_par = np.where(pxf_par == 0, fill, pxf_par)\n",
    "            pyf_par = np.where(pxf_par == 0, fill, pyf_par)\n",
    "            pzf_par = np.where(pxf_par == 0, fill, pzf_par)\n",
    "            \n",
    "        ef_par = ak.to_awkward0(ef_par) \n",
    "        ke_par = ak.to_awkward0(ke_par)    \n",
    "        cthf_par = ak.to_awkward0(cthf_par)    \n",
    "        \n",
    "        if(isMom):\n",
    "            pxf_par = ak.to_awkward0(pxf_par)\n",
    "            pyf_par = ak.to_awkward0(pyf_par)\n",
    "            pzf_par = ak.to_awkward0(pzf_par)\n",
    "        \n",
    "    \n",
    "        ## Add the jagged arrays of Ef, pxf, pyf, pzf, for each particle ##\n",
    "        if is_all:\n",
    "            \n",
    "            ef_pars =  ef_par[~(ef_par == fill)]\n",
    "            ke_pars =  ke_par[~(ke_par == fill)]\n",
    "            cthf_pars =  cthf_par[~(cthf_par == fill)]\n",
    "            \n",
    "            if(isMom):\n",
    "                pxf_pars = pxf_par[~(pxf_par == fill)]\n",
    "                pyf_pars = pyf_par[~(pyf_par == fill)]\n",
    "                pzf_pars = pzf_par[~(pzf_par == fill)]\n",
    "            \n",
    "            df_new['ef_%s'%(str(par))] = ak.to_numpy(ef_pars) \n",
    "            df_new['ke_%s'%(str(par))] = ak.to_numpy(ke_pars) \n",
    "            df_new['cthf_%s'%(str(par))] = ak.to_numpy(cthf_pars) \n",
    "            \n",
    "            if(isMom):\n",
    "                df_new['pxf_%s'%(str(par))] = ak.to_numpy(pxf_pars)\n",
    "                df_new['pyf_%s'%(str(par))] = ak.to_numpy(pyf_pars)\n",
    "                df_new['pzf_%s'%(str(par))] = ak.to_numpy(pzf_pars)\n",
    "       \n",
    "        ## Find the leading particle sorted base on energy in Ef, pxf pyf, pzf, ptf, and cal ##\n",
    "        if is_leading:\n",
    "            df_new['ef_%s_l'%(str(par))] = ef_par[::, 0]\n",
    "            df_new['ke_%s_l'%(str(par))] = ke_par[::, 0]\n",
    "            df_new['ke_%s_l'%(str(par))] = cthf_par[::, 0]\n",
    "            \n",
    "            if(isMom):\n",
    "                df_new['pxf_%s_l'%(str(par))] = pxf_par[::, 0]\n",
    "                df_new['pyf_%s_l'%(str(par))] = pyf_par[::, 0]\n",
    "                df_new['pzf_%s_l'%(str(par))] = pzf_par[::, 0]\n",
    "                df_new['ptf_%s_l'%(str(par))] = np.sqrt(pxf_par[::, 0]**2 +  pyf_par[::, 0]**2)\n",
    "\n",
    "        \n",
    "        # Count the number of outgoing particles in each event\n",
    "        if is_count:\n",
    "            # Remove fill values \n",
    "            ke_pars = ke_par[~(ke_par == fill)]\n",
    "            counts = ak.num(ke_pars)\n",
    "            df_new['%s_count'%(str(par))] = counts\n",
    "        \n",
    "        print('Done with: ', str(par), 'time took: ', time.time() - start_time, ' still working ...')\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataframe with subentires to find leading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdf(num):\n",
    "    ''' Read in necessary dataframes (fp: final particles, ip: initial particles, sm all other columns) '''\n",
    "    \n",
    "    gst_fp_df = pd.read_hdf(\"/Users/laurazichi/Desktop/Fermilab/run_%s_gntp_FSI_df.hdf\"%(num), \"gst_fp_df\")\n",
    "    gst_ip_df = pd.read_hdf(\"/Users/laurazichi/Desktop/Fermilab/run_%s_gntp_FSI_df.hdf\"%(num), \"gst_ip_df\")\n",
    "    gst_df = pd.read_hdf(\"/Users/laurazichi/Desktop/Fermilab/run_%s_gntp_FSI_df.hdf\"%(num), \"gst_df\")\n",
    "    \n",
    "    return gst_fp_df, gst_ip_df, gst_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wghts used ##\n",
    "wghts = ['wght_FrCEx_N_n1', 'wght_FrCEx_N_p1', 'wght_FrAbs_N_n1', 'wght_FrAbs_N_p1', \n",
    "         'wght_FrInel_N_n1', 'wght_FrInel_N_p1', 'wght_FrPiProd_N_n1', 'wght_FrPiProd_N_p1',\n",
    "         'wght_FrCEx_pi_n1', 'wght_FrCEx_pi_p1', 'wght_FrAbs_pi_n1', 'wght_FrAbs_pi_p1', \n",
    "         'wght_FrInel_pi_n1', 'wght_FrInel_pi_p1', 'wght_FrPiProd_pi_n1', 'wght_FrPiProd_pi_p1', 'wght']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_leading(df, pars):\n",
    "    '''For a given multindex dataframe, return a multindex dataframe of the leading particle KE and angle\n",
    "    for each particle in the pars array'''\n",
    "    df_new = pd.DataFrame()\n",
    "    \n",
    "    for par in pars: \n",
    "        ## Only works for outgoing particle pdgf change to pdgi if incoming \n",
    "        df_temp = df.query('pdgf == %s'%(par)).sort_index()\n",
    "        \n",
    "        # Find leading particle based on Ef\n",
    "        idx = df_temp.groupby(level = 0, sort = True)['Ef'].transform(max) == df_temp['Ef']\n",
    "        df_group = df_temp[idx]\n",
    "        \n",
    "        # Find count of particles\n",
    "        ser = df_temp.groupby(level=0, sort = True).agg({'Ef': 'count'}).rename(columns={'Ef':'Count'})  \n",
    "        df_group = df_group.merge(right = ser, right_index = True, left_index = True)\n",
    "        \n",
    "        # Add to new dataframe for return\n",
    "        df_new = pd.concat([df_new, df_group], sort = False)\n",
    "\n",
    "    df_new = df_new.sort_index()\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply cuts to Dataframes with subentires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in dataframe, make cuts, merge for one final dataframe ##\n",
    "def process_df(num):\n",
    "    \n",
    "    gst_all_fp = pd.DataFrame()\n",
    "    gst_all_cuts = pd.DataFrame()\n",
    "    \n",
    "    # Go through all others and cut and merge together\n",
    "    for x in np.arange(1, num+1, 1):\n",
    "        # Read in each dataframe\n",
    "        gst_all_in_temp = read_hdf(x)\n",
    "        \n",
    "        gst_all_fp_temp = gst_all_in_temp[0]\n",
    "        gst_all_temp = gst_all_in_temp[2]\n",
    "        \n",
    "        # Make cuts\n",
    "        # Lepton pt cut \n",
    "        gst_all_temp[\"ptl\"] = np.sqrt(gst_all_temp[\"pxl\"]**2+gst_all_temp[\"pyl\"]**2)\n",
    "        gst_all_fp_temp = gst_all_fp_temp.join(gst_all_temp[wghts + [\"ptl\"]],on='entry').query(\"ptl>0.4\")\n",
    "\n",
    "        # Zero cut \n",
    "        gst_all_fp_temp = gst_all_fp_temp.query('pxf != 0')\n",
    "\n",
    "        # All cuts df (kinetic energy and angle cuts)\n",
    "        gst_all_cuts_temp = gst_all_fp_temp.query('kef > 0.06').query('cthf > %s'%(np.cos(40*np.pi/180)))\n",
    "        \n",
    "        # Merge with others\n",
    "        gst_all_fp = pd.concat([gst_all_fp, gst_all_fp_temp], sort = True)\n",
    "        gst_all_cuts = pd.concat([gst_all_cuts, gst_all_cuts_temp], sort = True)\n",
    "        \n",
    "        print('Done with: %s Still working ...'%(x))\n",
    "    \n",
    "    return gst_all_fp, gst_all_cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with: 1 Still working ...\n",
      "Done with: 2 Still working ...\n",
      "Done with: 3 Still working ...\n",
      "Done with: 4 Still working ...\n",
      "Done with: 5 Still working ...\n",
      "Done with: 6 Still working ...\n",
      "Done with: 7 Still working ...\n",
      "Done with: 8 Still working ...\n",
      "Done with: 9 Still working ...\n",
      "Done with: 10 Still working ...\n"
     ]
    }
   ],
   "source": [
    "gst_all_fp, gst_all_cuts = process_df(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "leading_df = calc_leading(gst_all_fp, np.array([211, -211, 111, 2112, 2212]))\n",
    "leading_cuts_df = calc_leading(gst_all_cuts, np.array([211, -211, 111, 2112, 2212]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_all_fp.to_hdf('gst_fp_10', 'fp_10')\n",
    "gst_all_cuts.to_hdf('gst_10', 'gst_10')\n",
    "\n",
    "leading_df.to_hdf('lead_10', 'l_10')\n",
    "leading_cuts_df.to_hdf('lead_cuts_10', 'l_cuts10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
